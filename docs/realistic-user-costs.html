<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>NarraFlow - Internal Docs</title>
  <style>
    /* NarraFlow Brand Colors - Dark Theme with Blue Accent */
    :root {
      /* Radix UI Blue (Dark Mode) */
      --blue-1: #0d1520;
      --blue-2: #111927;
      --blue-3: #0d2847;
      --blue-4: #003362;
      --blue-5: #004074;
      --blue-6: #104d87;
      --blue-7: #205d9e;
      --blue-8: #2870bd;
      --blue-9: #0090ff;
      --blue-10: #3b9eff;
      --blue-11: #70b8ff;
      --blue-12: #c2e6ff;

      /* Radix UI Slate (Dark Mode) */
      --slate-1: #111113;
      --slate-2: #18191b;
      --slate-3: #212225;
      --slate-4: #272a2d;
      --slate-5: #2e3135;
      --slate-6: #363a3f;
      --slate-7: #43484e;
      --slate-8: #5a6169;
      --slate-9: #696e77;
      --slate-10: #777b84;
      --slate-11: #b0b4ba;
      --slate-12: #edeef0;
    }

    * {
      margin: 0;
      padding: 0;
      box-sizing: border-box;
    }

    body {
      font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
      line-height: 1.6;
      color: var(--slate-12);
      background: var(--slate-1);
      -webkit-font-smoothing: antialiased;
      -moz-osx-font-smoothing: grayscale;
      display: flex;
      height: 100vh;
      overflow: hidden;
    }

    /* Sidebar */
    .sidebar {
      width: 260px;
      background: var(--slate-2);
      border-right: 1px solid var(--slate-4);
      display: flex;
      flex-direction: column;
      transition: width 0.3s ease, margin-left 0.3s ease;
      position: relative;
      z-index: 10;
    }

    .sidebar.collapsed {
      width: 0;
      margin-left: -1px;
      overflow: hidden;
    }

    .sidebar-toggle {
      position: fixed;
      top: 20px;
      left: 272px;
      width: 32px;
      height: 32px;
      background: var(--slate-3);
      border: 1px solid var(--slate-4);
      border-radius: 6px;
      display: flex;
      align-items: center;
      justify-content: center;
      cursor: pointer;
      color: var(--slate-11);
      transition: all 0.3s ease;
      z-index: 100;
    }

    .sidebar-toggle:hover {
      background: var(--slate-4);
      color: var(--blue-11);
    }

    .sidebar.collapsed ~ .sidebar-toggle,
    .sidebar.collapsed .sidebar-toggle {
      left: 20px;
    }

    .sidebar-header {
      padding: 20px;
      border-bottom: 1px solid var(--slate-4);
    }

    .sidebar-header h2 {
      font-size: 18px;
      margin: 0;
      color: var(--slate-12);
      border: none;
      padding: 0;
    }

    .sidebar-nav {
      flex: 1;
      overflow-y: auto;
      padding: 10px 0;
    }

    .nav-item {
      padding: 10px 20px;
      color: var(--slate-11);
      cursor: pointer;
      transition: all 0.2s;
      border-left: 3px solid transparent;
      user-select: none;
    }

    .nav-item:hover {
      background: var(--slate-3);
      color: var(--slate-12);
    }

    .nav-item.active {
      background: var(--slate-3);
      color: var(--blue-11);
      border-left-color: var(--blue-9);
    }

    .nav-category {
      padding: 15px 20px 5px;
      font-size: 12px;
      font-weight: 600;
      color: var(--slate-10);
      text-transform: uppercase;
      letter-spacing: 0.5px;
    }

    /* Main content area */
    .main-wrapper {
      flex: 1;
      display: flex;
      flex-direction: column;
      overflow: hidden;
      transition: margin-left 0.3s ease;
    }

    .main-content {
      flex: 1;
      overflow-y: auto;
      padding: 20px;
    }

    .container {
      max-width: 1200px;
      margin: 0 auto;
      background: var(--slate-2);
      padding: 40px;
      border-radius: 12px;
      border: 1px solid var(--slate-4);
    }

    .page {
      display: none;
    }

    .page.active {
      display: block;
    }

    h1 {
      color: var(--slate-12);
      margin-bottom: 10px;
      font-size: 32px;
      font-weight: 700;
    }

    h2 {
      color: var(--slate-12);
      margin-top: 40px;
      margin-bottom: 20px;
      font-size: 24px;
      border-bottom: 2px solid var(--blue-9);
      padding-bottom: 10px;
      font-weight: 600;
    }

    h3 {
      color: var(--slate-11);
      margin-top: 30px;
      margin-bottom: 15px;
      font-size: 20px;
      font-weight: 600;
    }

    .subtitle {
      color: var(--slate-11);
      font-size: 16px;
      margin-bottom: 30px;
    }

    .highlight {
      background: var(--blue-3);
      padding: 20px;
      border-radius: 8px;
      margin: 20px 0;
      border-left: 4px solid var(--blue-9);
    }

    .highlight strong {
      color: var(--blue-11);
      font-size: 18px;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      margin: 20px 0;
      font-size: 14px;
      background: var(--slate-3);
      border-radius: 8px;
      overflow: hidden;
    }

    th {
      background: var(--slate-4);
      color: var(--slate-12);
      padding: 12px;
      text-align: left;
      font-weight: 600;
      border-bottom: 2px solid var(--blue-9);
    }

    td {
      padding: 10px 12px;
      border-bottom: 1px solid var(--slate-4);
      color: var(--slate-11);
    }

    tr:last-child td {
      border-bottom: none;
    }

    tr:hover {
      background: var(--slate-4);
    }

    .positive {
      color: #10b981;
      font-weight: 600;
    }

    .emphasis {
      color: var(--blue-11);
      font-weight: 600;
    }

    .groq-highlight {
      background: var(--blue-4);
      font-weight: 600;
    }

    .groq-highlight td {
      color: var(--blue-11);
    }

    .note {
      background: var(--slate-3);
      padding: 15px;
      border-radius: 8px;
      margin: 20px 0;
      border-left: 4px solid var(--blue-9);
      font-size: 14px;
      color: var(--slate-11);
    }

    .note strong {
      color: var(--slate-12);
    }

    ul {
      margin: 15px 0;
      padding-left: 30px;
      color: var(--slate-11);
    }

    li {
      margin: 8px 0;
    }

    .updated {
      color: var(--slate-10);
      font-size: 12px;
      margin-top: 30px;
      padding-top: 20px;
      border-top: 1px solid var(--slate-4);
      text-align: center;
    }

    /* Smooth scrolling */
    html {
      scroll-behavior: smooth;
    }

    /* Selection color */
    ::selection {
      background-color: var(--blue-9);
      color: white;
    }
  </style>
</head>
<body>
  <!-- Sidebar -->
  <div class="sidebar" id="sidebar">
    <div class="sidebar-header">
      <h2>NarraFlow Docs</h2>
    </div>
    <nav class="sidebar-nav">
      <div class="nav-item active" data-page="user-costs">Usage & Costs</div>
      <div class="nav-item" data-page="whisper-models">Whisper Models & Infrastructure</div>
      <div class="nav-item" data-page="implementation-plan">Implementation Plan</div>
      <div class="nav-item" data-page="post-processing">Post-Processing</div>
    </nav>
  </div>

  <!-- Floating Toggle Button -->
  <div class="sidebar-toggle" id="sidebar-toggle">
    <span id="toggle-icon">◀</span>
  </div>

  <!-- Main Content -->
  <div class="main-wrapper">
    <div class="main-content">
      <!-- Page: User Costs -->
      <div class="page active" id="page-user-costs">
        <div class="container">
    <h1>Realistic User Usage & Costs Analysis</h1>
    <p class="subtitle">Understanding actual user behavior and infrastructure costs for NarraFlow</p>

    <div class="highlight">
      <strong>Current Configuration:</strong>
      <div id="current-config" style="margin-top: 10px;"></div>
    </div>

    <div style="margin: 30px 0; padding: 20px; background: var(--slate-3); border-radius: 8px; border: 1px solid var(--slate-4);">
      <div style="display: flex; align-items: center; gap: 20px; margin-bottom: 15px;">
        <label style="display: flex; align-items: center; gap: 10px; cursor: pointer;">
          <input type="checkbox" id="include-formatting" style="width: 18px; height: 18px; cursor: pointer;">
          <strong style="color: var(--slate-12);">Include Post-Processing Costs</strong>
        </label>
      </div>

      <div id="formatting-options" style="display: none; margin-top: 15px; padding-top: 15px; border-top: 1px solid var(--slate-4);">
        <div style="margin-bottom: 10px;">
          <label style="color: var(--slate-11); display: block; margin-bottom: 8px;"><strong>Formatting Model:</strong></label>
          <select id="formatting-model" style="padding: 8px 12px; background: var(--slate-2); border: 1px solid var(--slate-4); border-radius: 6px; color: var(--slate-12); font-size: 14px; width: 300px;">
            <option value="llama8b">Llama 3.1 8B Instant (Current)</option>
            <option value="llama70b">Llama 3.3 70B Versatile</option>
            <option value="gpt4oMini">GPT-4o-mini</option>
          </select>
        </div>

        <div style="margin-top: 15px; font-size: 13px; color: var(--slate-10);">
          <strong>Assumptions:</strong> Avg recording length: <span id="avg-recording-length">30</span>s | Avg tokens: <span id="avg-tokens">150</span> input, <span id="avg-output-tokens">200</span> output
        </div>
      </div>
    </div>

    <h2>Text Formatting Model Comparison</h2>
    <p class="subtitle">Cost and performance analysis for Llama post-processing models</p>

    <div class="note">
      <strong>Context:</strong> After Whisper transcription, we optionally run text through Llama for cleanup (punctuation, capitalization, filler word removal). Typical formatting request: ~150 input tokens (raw text), ~200 output tokens (formatted text).
    </div>

    <table id="llama-comparison-table">
      <thead>
        <tr>
          <th>Model</th>
          <th>Input Price</th>
          <th>Output Price</th>
          <th>Cost per Format</th>
          <th>Speed</th>
          <th>Quality</th>
        </tr>
      </thead>
      <tbody id="llama-comparison-body"></tbody>
    </table>

    <div class="note">
      <strong>Cost Impact:</strong> Even with 100% formatting enabled for all transcriptions, formatting costs are negligible compared to transcription costs. Quality and instruction-following are the key differentiators.
    </div>

    <h2>User Segment Distribution</h2>
    <table id="user-segments-table">
      <thead>
        <tr>
          <th>Segment</th>
          <th>Hours/Month</th>
          <th>% of Users</th>
        </tr>
      </thead>
      <tbody id="user-segments-body"></tbody>
    </table>
    <div id="blended-average"></div>

    <h2>Cost Analysis by User Segment</h2>
    <div id="segment-costs"></div>

    <h2>Blended Cost Analysis</h2>
    <div class="note">
      <strong>Note:</strong> Blended cost represents the weighted average across all user segments based on their distribution.
    </div>
    <table id="blended-costs-table">
      <thead>
        <tr>
          <!-- Dynamically populated by JavaScript -->
        </tr>
      </thead>
      <tbody id="blended-costs-body"></tbody>
    </table>

    <h2>Business Projections</h2>
    <div id="business-summary"></div>

    <h3>Revenue & Costs at Scale</h3>
    <table id="scale-table">
      <thead>
        <tr>
          <th>Provider</th>
          <th>Monthly Revenue</th>
          <th>Monthly Costs</th>
          <th>Monthly Profit</th>
          <th>Annual Profit</th>
          <th>Margin</th>
        </tr>
      </thead>
      <tbody id="scale-body"></tbody>
    </table>

    <div class="updated">
      Last updated: <span id="last-updated"></span> |
      Pricing based on official Groq API documentation
    </div>
        </div>
      </div>

      <!-- Page: Whisper Models & Infrastructure -->
      <div class="page" id="page-whisper-models">
        <div class="container">
          <h1>Whisper Models & Infrastructure</h1>
          <p class="subtitle">Comparison of Whisper implementations and deployment options</p>

          <h2>Cloud Providers (Current)</h2>
          <table id="cloud-providers-table">
            <thead>
              <tr>
                <th>Provider</th>
                <th>Model</th>
                <th>Cost/Hour Audio</th>
                <th>Speed</th>
                <th>Notes</th>
              </tr>
            </thead>
            <tbody id="cloud-providers-body"></tbody>
          </table>

          <h2>Local Processing Options (Privacy Mode)</h2>
          <p class="subtitle">Based on November 2025 benchmarks - For users who want 100% local transcription</p>

          <table>
            <thead>
              <tr>
                <th>Implementation</th>
                <th>Model</th>
                <th>Hardware</th>
                <th>30s Audio</th>
                <th>60s Audio</th>
                <th>2min Audio</th>
                <th><span title="Word Error Rate - Lower is better. Human transcription: ~4%. Automated systems: 5-15%. Varies by audio quality, accent, background noise, and domain." style="cursor: help; border-bottom: 1px dotted var(--slate-11);">Quality (WER)</span></th>
                <th>Notes</th>
              </tr>
            </thead>
            <tbody>
              <tr class="groq-highlight">
                <td><strong>Apple Native APIs (SpeechAnalyzer)</strong></td>
                <td>Proprietary (iOS 26/macOS Tahoe)</td>
                <td>ANE + GPU (All Apple Silicon)</td>
                <td>~0.7s</td>
                <td>~1.3s</td>
                <td>~2.7s</td>
                <td>Excellent (~5%)</td>
                <td>55% faster than Whisper V3 Turbo - Requires macOS 26+ (Sept 2025)</td>
              </tr>
              <tr class="groq-highlight">
                <td><strong>WhisperKit</strong></td>
                <td>large-v3</td>
                <td>ANE + GPU (M1-M4)</td>
                <td>~1.5s</td>
                <td>~3s</td>
                <td>~6s</td>
                <td>Excellent (~5%)</td>
                <td>Production-proven (ModMed), 0.45s streaming latency</td>
              </tr>
              <tr>
                <td><strong>Lightning Whisper MLX</strong></td>
                <td>distil-large-v3</td>
                <td>GPU (M1-M4)</td>
                <td>~2s</td>
                <td>~4s</td>
                <td>~8s</td>
                <td>Very Good (~6-7%)</td>
                <td>10x faster than whisper.cpp, Python-based</td>
              </tr>
              <tr>
                <td><strong>MLX Whisper</strong></td>
                <td>large-v3</td>
                <td>GPU (M1-M4)</td>
                <td>~3s</td>
                <td>~6s</td>
                <td>~12s</td>
                <td>Excellent (~5%)</td>
                <td>30-40% faster than alternatives, Python-based</td>
              </tr>
            </tbody>
          </table>

          <h2>Model Sizes & Download Requirements</h2>
          <p class="subtitle">Applies to WhisperKit, Lightning Whisper MLX, and MLX Whisper (not Apple Native APIs)</p>
          <table>
            <thead>
              <tr>
                <th>Model</th>
                <th>Parameters</th>
                <th>Disk Size</th>
                <th>RAM Usage</th>
                <th>Quality (WER)</th>
                <th>Use Case</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td><strong>tiny.en</strong></td>
                <td>39M</td>
                <td>~75MB</td>
                <td>~150MB</td>
                <td>Poor (~12-15%)</td>
                <td>Testing only - hallucination issues</td>
              </tr>
              <tr>
                <td><strong>base.en</strong></td>
                <td>74M</td>
                <td>~140MB</td>
                <td>~300MB</td>
                <td>Good (~8-10%)</td>
                <td>Fast download, decent quality</td>
              </tr>
              <tr>
                <td><strong>small.en</strong></td>
                <td>244M</td>
                <td>~460MB</td>
                <td>~1GB</td>
                <td>Very Good (~6-7%)</td>
                <td>Quality/speed balance</td>
              </tr>
              <tr>
                <td><strong>medium.en</strong></td>
                <td>769M</td>
                <td>~1.5GB</td>
                <td>~3GB</td>
                <td>Excellent (~5-6%)</td>
                <td>High quality local</td>
              </tr>
              <tr>
                <td><strong>large-v3</strong></td>
                <td>1550M</td>
                <td>~3GB</td>
                <td>~6GB</td>
                <td>Best (~4-5%)</td>
                <td>Maximum quality (cloud or local)</td>
              </tr>
              <tr>
                <td><strong>distil-large-v3</strong></td>
                <td>756M</td>
                <td>~1.5GB</td>
                <td>~3GB</td>
                <td>Excellent (~5-6%)</td>
                <td>Faster than large-v3, similar quality</td>
              </tr>
            </tbody>
          </table>

          <h2>Implementation Details</h2>

          <h3>Apple Native APIs (SpeechAnalyzer)</h3>
          <p><strong>What it is:</strong> Apple's new SpeechAnalyzer API introduced in iOS 26/macOS Tahoe (released September 15, 2025). Replaces legacy SFSpeechRecognizer with major improvements.</p>
          <p><strong>Availability:</strong> ✅ CONFIRMED available to third-party apps. No Siri enablement required (unlike legacy SFSpeechRecognizer).</p>
          <p><strong>Performance:</strong> 2.2x faster than competitors. 55% faster than Whisper Large V3 Turbo. Processes 34-minute video in 45 seconds. 100% on-device processing.</p>
          <p><strong>Pros:</strong> Fastest option, no model downloads, built into OS, excellent quality, auto-updating models, smallest app size, instant first launch.</p>
          <p><strong>Cons:</strong> Requires macOS 26+ (limits user base to 2-month-old OS), proprietary (no model control), users may wait 3-6 months before updating.</p>
          <p><strong>Integration:</strong> Available via Swift native module in Electron using Objective-C++ bridge. Official Electron docs support this pattern. Uses modern async/await with AsyncStream.</p>

          <h3>WhisperKit</h3>
          <p><strong>What it is:</strong> Production-grade Whisper implementation by Argmax, optimized for Apple Neural Engine.</p>
          <p><strong>Performance:</strong> 0.45s mean latency for streaming transcription. Real-time capable with near-peak hardware utilization. 2x slower than SpeechAnalyzer but still very fast.</p>
          <p><strong>Pros:</strong> Battle-tested in production (ModMed medical app), works on macOS 11+ (M1+), energy efficient, excellent quality, open source (model control).</p>
          <p><strong>Cons:</strong> Requires ~3GB model download, requires Swift native module for Electron (same as SpeechAnalyzer), larger app bundle.</p>
          <p><strong>Integration:</strong> Swift Package via Objective-C++ bridge in Electron. Same integration pattern as SpeechAnalyzer. Open-sourced January 2024 under MIT license.</p>

          <h3>Lightning Whisper MLX</h3>
          <p><strong>What it is:</strong> Highly optimized Whisper implementation using Apple's MLX framework with batched decoding and quantization.</p>
          <p><strong>Performance:</strong> 10x faster than whisper.cpp, 4x faster than standard MLX Whisper. Uses distilled models for speed.</p>
          <p><strong>Pros:</strong> Very fast, Python-based (easy integration), optimized for Apple Silicon, open-source.</p>
          <p><strong>Cons:</strong> Requires M1 or later, Python dependency.</p>
          <p><strong>Integration:</strong> Python package, can be called from Electron via subprocess or Python bridge.</p>

          <h3>MLX Whisper</h3>
          <p><strong>What it is:</strong> Standard Whisper implementation using Apple's MLX framework for Metal GPU acceleration.</p>
          <p><strong>Performance:</strong> 30-40% faster than CPU-only implementations. 1.78x faster than whisper.cpp on M1 Max.</p>
          <p><strong>Pros:</strong> Good performance, Python-based (easy integration), works well with longer audio (32-40min), mature.</p>
          <p><strong>Cons:</strong> Slower than Lightning Whisper MLX and WhisperKit, requires M1 or later.</p>
          <p><strong>Integration:</strong> Python package (`mlx-whisper`), can be called from Electron via subprocess or Python bridge.</p>

          <h2>Recommended Approach for NarraFlow</h2>
          <div class="highlight">
            <strong>Hybrid Strategy: Best of All Worlds</strong><br><br>
            Automatically select the best transcription method based on user's system:
            <ul style="margin-top: 10px;">
              <li><strong>macOS 26+:</strong> Use SpeechAnalyzer (fastest, no downloads, 0.7s for 30s audio)</li>
              <li><strong>macOS 11+ (M1+):</strong> Use WhisperKit (fast, proven, 1.5s for 30s audio)</li>
              <li><strong>Fallback:</strong> Use Groq API (cloud, works on all systems)</li>
            </ul>
          </div>
          <p><strong>Detection:</strong> Electron can easily detect macOS version via <code>process.getSystemVersion()</code>. Check if version >= 26 for SpeechAnalyzer, else check for M1+ for WhisperKit.</p>
          <p><strong>User Experience:</strong> macOS 26 users get instant, ultra-fast transcription with zero setup. Older Mac users still get excellent local performance. Everyone can fall back to cloud if needed.</p>
          <p><strong>Market Coverage:</strong> WhisperKit supports Macs from 2020+ (M1 launch), covering vast majority of potential users while giving cutting-edge performance to latest OS adopters.</p>
        </div>
      </div>

      <!-- Page: Implementation Plan -->
      <div class="page" id="page-implementation-plan">
        <div class="container">
          <h1>NarraFlow Transcription: Implementation Plan</h1>
          <p class="subtitle">Official strategy for local-first transcription with intelligent fallbacks</p>

          <div class="highlight">
            <strong>Core Philosophy:</strong> Local-first for privacy and zero marginal costs, with intelligent cloud fallback only when absolutely necessary.
          </div>

          <h2>Architecture Overview</h2>

          <h3>Transcription Backbone: WhisperKit</h3>
          <p><strong>Primary Engine:</strong> WhisperKit serves as the foundation for all local transcription.</p>
          <ul>
            <li><strong>Coverage:</strong> Works on ALL Macs (Apple Silicon + Intel)</li>
            <li><strong>Apple Silicon:</strong> Fast (~1.5s for 30s audio) via Apple Neural Engine</li>
            <li><strong>Intel Macs:</strong> Acceptable (~3-4s for 30s audio) via GPU acceleration</li>
            <li><strong>Integration:</strong> Swift native module via Objective-C++ bridge in Electron</li>
            <li><strong>Models:</strong> Ships with large-v3 (~3GB download on first launch)</li>
          </ul>

          <h3>Performance Boost: SpeechAnalyzer (macOS 26+)</h3>
          <p><strong>Optional Enhancement:</strong> When available, SpeechAnalyzer provides cutting-edge performance.</p>
          <ul>
            <li><strong>Speed:</strong> 2x faster than WhisperKit (~0.7s for 30s audio)</li>
            <li><strong>Zero Setup:</strong> Built into OS, no model downloads</li>
            <li><strong>Requirement:</strong> macOS 26+ (released Sept 2025)</li>
            <li><strong>Detection:</strong> Check <code>process.getSystemVersion() >= 26</code></li>
          </ul>

          <h2>Intelligent Fallback Strategy</h2>

          <h3>Decision Flow</h3>
          <div class="highlight">
            <strong>Step 1: OS Version Check</strong><br>
            If macOS 26+ detected → Use SpeechAnalyzer (fastest)<br>
            Else → Use WhisperKit (backbone)
          </div>

          <div class="highlight" style="margin-top: 15px;">
            <strong>Step 2: Performance Monitoring</strong><br>
            Measure WhisperKit transcription time on first use<br>
            If duration > 3s for 30s audio → Flag as "slow hardware"
          </div>

          <div class="highlight" style="margin-top: 15px;">
            <strong>Step 3: Adaptive Cloud Fallback</strong><br>
            For "slow hardware" users → Automatically switch to Groq API<br>
            Store preference locally, notify user of the switch
          </div>

          <h3>Performance Thresholds</h3>
          <table>
            <thead>
              <tr>
                <th>Audio Length</th>
                <th>Acceptable (Local)</th>
                <th>Too Slow (Switch to Cloud)</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>30 seconds</td>
                <td>≤ 3s</td>
                <td>> 3s</td>
              </tr>
              <tr>
                <td>60 seconds</td>
                <td>≤ 6s</td>
                <td>> 6s</td>
              </tr>
              <tr>
                <td>120 seconds</td>
                <td>≤ 12s</td>
                <td>> 12s</td>
              </tr>
            </tbody>
          </table>

          <h2>First Launch Experience</h2>
          <div class="note">
            <strong>SpeechAnalyzer (macOS 26+):</strong> Instant - no downloads, built into OS<br>
            <strong>WhisperKit (other):</strong> One-time ~3GB model download (5-10 min on good connection)<br>
            <strong>Groq fallback:</strong> Works immediately, no setup required
          </div>

          <h2>Projected User Distribution</h2>

          <table>
            <thead>
              <tr>
                <th>User Segment</th>
                <th>Engine</th>
                <th>% of Users</th>
              </tr>
            </thead>
            <tbody>
              <tr class="groq-highlight">
                <td>macOS 26+ (early adopters)</td>
                <td>SpeechAnalyzer</td>
                <td>15-20%</td>
              </tr>
              <tr class="groq-highlight">
                <td>Apple Silicon (M1-M4)</td>
                <td>WhisperKit</td>
                <td>65-70%</td>
              </tr>
              <tr>
                <td>Intel (fast enough)</td>
                <td>WhisperKit</td>
                <td>5-10%</td>
              </tr>
              <tr>
                <td>Intel (too slow)</td>
                <td>Groq (cloud)</td>
                <td>5-10%</td>
              </tr>
            </tbody>
          </table>

          <div class="note">
            <strong>Bottom Line:</strong> 90-95% of users will use local processing (zero infrastructure costs, complete privacy)
          </div>

          <div class="updated">
            Last updated: <span id="plan-last-updated"></span> |
            This is the official implementation plan for NarraFlow transcription
          </div>
        </div>
      </div>

      <!-- Page: Post-Processing -->
      <div class="page" id="page-post-processing">
        <div class="container">
          <h1>Post-Processing: Text Formatting</h1>
          <p class="subtitle">Cleaning up transcription output with punctuation, capitalization, and formatting</p>

          <div class="highlight">
            <strong>Current Status:</strong> No post-processing implemented. Whisper output is used directly.<br>
            <strong>Future Plan:</strong> Add intelligent text formatting to improve transcription quality.
          </div>

          <h2>The Challenge</h2>

          <p>Whisper transcription is excellent but produces raw text without proper formatting:</p>
          <ul>
            <li>Missing capitalization at sentence starts</li>
            <li>Inconsistent punctuation (periods, commas, question marks)</li>
            <li>No paragraph breaks or structure</li>
            <li>Filler words (um, uh, like) not removed</li>
          </ul>

          <p><strong>Goal:</strong> Transform raw transcription into polished, properly formatted text.</p>

          <h2>Option 1: Apple Foundation Models (macOS 26+)</h2>

          <h3>What It Is</h3>
          <p>Apple's new Foundation Models framework (WWDC 2025) provides access to the on-device ~3B parameter LLM that powers Apple Intelligence.</p>

          <h3>Key Features</h3>
          <ul>
            <li><strong>Built into OS:</strong> No model downloads, zero setup</li>
            <li><strong>Optimized for text tasks:</strong> Summarization, extraction, classification</li>
            <li><strong>Swift API:</strong> Same integration pattern as SpeechAnalyzer</li>
            <li><strong>100% On-Device:</strong> Complete privacy, works offline</li>
            <li><strong>Zero Cost:</strong> Free to use, no inference fees</li>
          </ul>

          <h3>Integration</h3>
          <p>Since NarraFlow already uses Swift native modules for SpeechAnalyzer (macOS 26+), adding Foundation Models is straightforward - same Objective-C++ bridge pattern.</p>

          <h3>Unknown Performance</h3>
          <div class="note">
            <strong>To Be Tested:</strong> Apple Foundation Models are optimized for general text tasks, but we don't know yet if they can do <em>pure formatting</em> without trying to be "helpful" or answering prompts. This needs hands-on testing.
          </div>

          <h3>Availability</h3>
          <p><strong>Requirement:</strong> macOS 26+ only (same as SpeechAnalyzer)</p>
          <p>Users on older macOS versions would need a different solution.</p>

          <h2>Option 2: Fine-Tune Llama Locally</h2>

          <h3>Why Llama Failed Initially</h3>
          <p>Previous attempts with base Llama 3.1 8B (via Groq) failed because instruction-tuned models are trained to be "helpful assistants" - they want to answer questions and engage in conversation, not just format text. Even with explicit prompts saying "do not answer," the model would still try to respond to content instead of just cleaning formatting.</p>

          <div class="note">
            <strong>The Problem:</strong> General-purpose LLMs are optimized for chat/Q&A, not task-specific formatting. They need fine-tuning to learn this specific behavior.
          </div>

          <h3>The Solution: Local Fine-Tuning with MLX</h3>

          <p>Apple's MLX framework enables efficient fine-tuning of Llama models directly on Mac hardware using LoRA (Low-Rank Adaptation).</p>

          <h3>Time Requirements (M1 Max 64GB)</h3>
          <p><strong>Hardware:</strong> M1 Max (64GB unified memory)<br>
          <strong>Model:</strong> Llama 3.1 8B (same model as Groq uses)<br>
          <strong>Method:</strong> LoRA fine-tuning with MLX</p>

          <table>
            <thead>
              <tr>
                <th>Dataset Size</th>
                <th>Estimated Duration</th>
              </tr>
            </thead>
            <tbody>
              <tr class="groq-highlight">
                <td>1-2k samples</td>
                <td><strong>20-30 minutes</strong></td>
              </tr>
              <tr class="groq-highlight">
                <td>2-4k samples</td>
                <td><strong>40-60 minutes</strong></td>
              </tr>
              <tr class="groq-highlight">
                <td>4-5k samples</td>
                <td><strong>60-90 minutes</strong></td>
              </tr>
              <tr class="groq-highlight">
                <td>5-6k samples</td>
                <td><strong>90-120 minutes</strong></td>
              </tr>
            </tbody>
          </table>

          <div class="note">
            <strong>Note:</strong> Training time scales roughly linearly with dataset size. The M1 Max's 64GB unified memory allows for large batch sizes and efficient parallel processing, making it ideal for fine-tuning without needing cloud services.
          </div>

          <h3>Process Overview</h3>
          <div class="highlight">
            <strong>Step 1:</strong> Collect Training Data (1-2k examples)<br>
            → Raw transcription text → Properly formatted output<br><br>

            <strong>Step 2:</strong> Fine-tune Locally with MLX<br>
            → Use <code>mlx-lm</code> library with LoRA method<br>
            → Run on M1 Max (20-120 minutes depending on dataset size)<br><br>

            <strong>Step 3:</strong> Deploy Fine-Tuned Model<br>
            → Load via MLX in Electron app<br>
            → Works on all M1+ Macs, fully local
          </div>

          <h3>Resources</h3>
          <p>Extensive tutorials available (January-June 2025):</p>
          <ul>
            <li>MLX fine-tuning guides for Llama 3</li>
            <li>LoRA training on Mac with 16GB RAM</li>
            <li>Deployment with Ollama and MLX</li>
            <li>Community support via MLX GitHub and forums</li>
          </ul>

          <div class="updated">
            Last updated: <span id="postprocessing-last-updated"></span>
          </div>
        </div>
      </div>
    </div>
  </div>

  <script src="pricing-config.js"></script>
  <script>
    // Helper functions
    const formatCurrency = (amount) => {
      // Show cents for amounts under $1, otherwise whole dollars
      const decimals = amount < 1 ? 2 : 0;
      return new Intl.NumberFormat('en-US', {
        style: 'currency',
        currency: 'USD',
        minimumFractionDigits: decimals,
        maximumFractionDigits: decimals,
      }).format(amount);
    };
    const formatPercent = (decimal) => `${(decimal * 100).toFixed(0)}%`;

    // Calculate blended average hours
    function calculateBlendedAverage() {
      return PRICING_CONFIG.app.userSegments.reduce((sum, segment) => {
        return sum + (segment.hoursPerMonth * segment.percentage) / 100;
      }, 0);
    }

    // Calculate formatting cost per hour of audio
    function calculateFormattingCost(hours, modelKey) {
      const avgRecordingLengthSeconds = 30;
      const recordingsPerHour = (hours * 3600) / avgRecordingLengthSeconds;
      const avgInputTokens = 150;
      const avgOutputTokens = 200;

      const model = PRICING_CONFIG.groq[modelKey];
      const costPerFormatting =
        (avgInputTokens / 1000000) * model.pricePerMInputTokens +
        (avgOutputTokens / 1000000) * model.pricePerMOutputTokens;

      return recordingsPerHour * costPerFormatting;
    }

    // Calculate provider cost (whisper only)
    function calculateProviderCost(provider, hours) {
      if (provider.pricePerHour !== undefined) {
        // Direct pricing (Groq)
        return hours * provider.pricePerHour;
      } else {
        // GPU pricing
        const gpuHoursNeeded = hours / provider.speedMultiplier;
        return gpuHoursNeeded * provider.pricePerGpuHour;
      }
    }

    // Calculate total cost including optional formatting
    function calculateTotalCost(provider, hours, includeFormatting, formattingModel) {
      const whisperCost = calculateProviderCost(provider, hours);
      if (!includeFormatting) return whisperCost;

      const formattingCost = calculateFormattingCost(hours, formattingModel);
      return whisperCost + formattingCost;
    }

    // Build provider list
    function getProviders() {
      const providers = [];

      // Groq providers
      providers.push({
        name: PRICING_CONFIG.groq.whisperTurbo.name,
        pricePerHour: PRICING_CONFIG.groq.whisperTurbo.pricePerHour,
        isGroq: true
      });

      providers.push({
        name: PRICING_CONFIG.groq.whisperLarge.name,
        pricePerHour: PRICING_CONFIG.groq.whisperLarge.pricePerHour,
        isGroq: true
      });

      // Competitors
      for (const [platform, gpus] of Object.entries(PRICING_CONFIG.competitors)) {
        for (const gpu of Object.values(gpus)) {
          providers.push({
            name: gpu.name,
            pricePerGpuHour: gpu.pricePerGpuHour,
            speedMultiplier: gpu.speedMultiplier,
            isGroq: false
          });
        }
      }

      return providers;
    }

    // Initialize
    function init() {
      const config = PRICING_CONFIG;
      const blendedAvg = calculateBlendedAverage();
      const providers = getProviders();
      const payingUsers = Math.round(config.app.market.totalUsers * config.app.conversionRate);
      const monthlyRevenue = payingUsers * config.app.subscriptionPrice;

      // Get formatting settings
      const includeFormatting = document.getElementById('include-formatting').checked;
      const formattingModel = document.getElementById('formatting-model').value;

      // Current config
      document.getElementById('current-config').innerHTML = `
        <strong>Model:</strong> ${config.groq.whisperTurbo.name} (${config.groq.whisperTurbo.model})<br>
        <strong>Pricing:</strong> ${formatCurrency(config.groq.whisperTurbo.pricePerHour)}/hour of audio<br>
        <strong>Speed:</strong> ${config.groq.whisperTurbo.speedMultiplier}x real-time<br>
        <strong>Formatting:</strong> ${config.groq.llama8b.name} (current)<br>
        <strong>Subscription:</strong> ${formatCurrency(config.app.subscriptionPrice)}/month
      `;

      // Llama comparison table
      const llamaBody = document.getElementById('llama-comparison-body');
      llamaBody.innerHTML = ''; // Clear previous content
      const avgInputTokens = 150;  // Typical raw transcription text
      const avgOutputTokens = 200; // Formatted text (slightly longer with punctuation)

      const llamaModels = [
        {
          model: config.groq.llama8b,
          quality: 'Good',
          notes: 'Current - Fast but requires careful prompting'
        },
        {
          model: config.groq.llama70b,
          quality: 'Excellent',
          notes: 'Better instruction following, less hallucination'
        },
        {
          model: config.groq.gpt4oMini,
          quality: 'Excellent',
          notes: 'Best instruction following (requires OpenAI API)'
        }
      ];

      llamaModels.forEach(({ model, quality, notes }) => {
        const inputCost = (avgInputTokens / 1000000) * model.pricePerMInputTokens;
        const outputCost = (avgOutputTokens / 1000000) * model.pricePerMOutputTokens;
        const totalCost = inputCost + outputCost;
        const latency = Math.round((avgOutputTokens / model.tokensPerSecond) * 1000); // ms
        const isCurrentModel = model.model === config.groq.llama8b.model;

        llamaBody.innerHTML += `
          <tr${isCurrentModel ? ' class="groq-highlight"' : ''}>
            <td>
              <strong>${model.name}</strong><br>
              <span style="font-size: 12px; color: var(--slate-10);">${notes}</span>
            </td>
            <td>${formatCurrency(model.pricePerMInputTokens)}/M</td>
            <td>${formatCurrency(model.pricePerMOutputTokens)}/M</td>
            <td class="emphasis">${formatCurrency(totalCost)}</td>
            <td>~${latency}ms</td>
            <td class="${quality === 'Excellent' ? 'positive' : ''}">${quality}</td>
          </tr>
        `;
      });

      // Cloud Providers table
      const cloudProvidersBody = document.getElementById('cloud-providers-body');
      if (cloudProvidersBody) {
        cloudProvidersBody.innerHTML = ''; // Clear previous content

        const cloudProviders = [
          {
            provider: 'Groq',
            model: config.groq.whisperTurbo.name,
            costPerHour: config.groq.whisperTurbo.pricePerHour,
            speed: `${config.groq.whisperTurbo.speedMultiplier}x real-time`,
            notes: 'Current - Ultra-fast LPU inference',
            isCurrentProvider: true
          },
          {
            provider: 'Groq',
            model: config.groq.whisperLarge.name,
            costPerHour: config.groq.whisperLarge.pricePerHour,
            speed: `${config.groq.whisperLarge.speedMultiplier}x real-time`,
            notes: 'Higher quality, slightly slower',
            isCurrentProvider: false
          },
          {
            provider: 'OpenAI',
            model: config.competitors.openai.whisper.name,
            costPerHour: config.competitors.openai.whisper.pricePerHour,
            speed: `${config.competitors.openai.whisper.speedMultiplier}x real-time`,
            notes: 'Official API - slower & more expensive',
            isCurrentProvider: false
          }
        ];

        cloudProviders.forEach(({ provider, model, costPerHour, speed, notes, isCurrentProvider }) => {
          const rowClass = isCurrentProvider ? ' class="groq-highlight"' : '';
          cloudProvidersBody.innerHTML += `
            <tr${rowClass}>
              <td><strong>${provider}</strong></td>
              <td>${model}</td>
              <td class="emphasis">${formatCurrency(costPerHour)}</td>
              <td>${speed}</td>
              <td>${notes}</td>
            </tr>
          `;
        });
      }

      // User segments
      const segmentsBody = document.getElementById('user-segments-body');
      segmentsBody.innerHTML = ''; // Clear previous content
      config.app.userSegments.forEach(segment => {
        segmentsBody.innerHTML += `
          <tr>
            <td><strong>${segment.name}</strong></td>
            <td>${segment.hoursPerMonth}</td>
            <td>${segment.percentage}%</td>
          </tr>
        `;
      });

      // Blended average
      document.getElementById('blended-average').innerHTML = `
        <div class="highlight">
          <strong>Blended Average:</strong> ${blendedAvg.toFixed(1)} hours/month
        </div>
      `;

      // Segment costs
      const segmentCostsDiv = document.getElementById('segment-costs');
      segmentCostsDiv.innerHTML = ''; // Clear previous content

      config.app.userSegments.forEach(segment => {
        let tableHTML = `
          <h3>${segment.name} (${segment.hoursPerMonth} hours/month)</h3>
          <table>
            <thead>
              <tr>
                <th>Provider</th>
                <th>Whisper Cost</th>
                ${includeFormatting ? '<th>Formatting Cost</th>' : ''}
                <th>Total Cost</th>
                <th>Profit @ $${config.app.subscriptionPrice}</th>
                <th>Margin</th>
              </tr>
            </thead>
            <tbody>
        `;

        providers.forEach(provider => {
          const whisperCost = calculateProviderCost(provider, segment.hoursPerMonth);
          const formattingCost = includeFormatting ? calculateFormattingCost(segment.hoursPerMonth, formattingModel) : 0;
          const totalCost = whisperCost + formattingCost;
          const profit = config.app.subscriptionPrice - totalCost;
          const margin = profit / config.app.subscriptionPrice;
          const rowClass = provider.isGroq ? ' class="groq-highlight"' : '';

          tableHTML += `
            <tr${rowClass}>
              <td><strong>${provider.name}</strong></td>
              <td>${formatCurrency(whisperCost)}</td>
              ${includeFormatting ? `<td>${formatCurrency(formattingCost)}</td>` : ''}
              <td class="emphasis">${formatCurrency(totalCost)}</td>
              <td class="positive">${formatCurrency(profit)}</td>
              <td class="emphasis">${formatPercent(margin)}</td>
            </tr>
          `;
        });

        tableHTML += '</tbody></table>';
        segmentCostsDiv.innerHTML += tableHTML;
      });

      // Blended costs
      const blendedBody = document.getElementById('blended-costs-body');
      blendedBody.innerHTML = ''; // Clear previous content
      const blendedCosts = {};

      // Update table header
      const blendedTable = document.getElementById('blended-costs-table');
      blendedTable.querySelector('thead tr').innerHTML = `
        <th>Provider</th>
        <th>Whisper Cost</th>
        ${includeFormatting ? '<th>Formatting Cost</th>' : ''}
        <th>Total Cost/User</th>
        <th>Profit @ $<span id="pricing2">${config.app.subscriptionPrice}</span>/month</th>
        <th>Margin</th>
      `;

      providers.forEach(provider => {
        const whisperCost = calculateProviderCost(provider, blendedAvg);
        const formattingCost = includeFormatting ? calculateFormattingCost(blendedAvg, formattingModel) : 0;
        const totalCost = whisperCost + formattingCost;
        const profit = config.app.subscriptionPrice - totalCost;
        const margin = profit / config.app.subscriptionPrice;
        blendedCosts[provider.name] = totalCost;
        const rowClass = provider.isGroq ? ' class="groq-highlight"' : '';

        blendedBody.innerHTML += `
          <tr${rowClass}>
            <td><strong>${provider.name}</strong></td>
            <td>${formatCurrency(whisperCost)}</td>
            ${includeFormatting ? `<td>${formatCurrency(formattingCost)}</td>` : ''}
            <td class="emphasis">${formatCurrency(totalCost)}</td>
            <td class="positive">${formatCurrency(profit)}</td>
            <td class="emphasis">${formatPercent(margin)}</td>
          </tr>
        `;
      });

      // Business summary
      document.getElementById('business-summary').innerHTML = `
        <div class="highlight">
          <strong>Year 1 Target: 10% of Wispr Flow's Market</strong><br><br>
          <ul>
            <li>Total users: ${config.app.market.totalUsers.toLocaleString()} (${config.app.market.targetPercentage}% of ${config.app.market.wisprFlowUsers.toLocaleString()})</li>
            <li>Paying users: ${payingUsers.toLocaleString()} (${formatPercent(config.app.conversionRate)} conversion rate)</li>
            <li>Pricing: ${formatCurrency(config.app.subscriptionPrice)}/month</li>
            <li>Blended usage: ${blendedAvg.toFixed(1)} hours/month</li>
          </ul>
        </div>
      `;

      // Scale table
      const scaleBody = document.getElementById('scale-body');
      scaleBody.innerHTML = ''; // Clear previous content

      providers.forEach(provider => {
        const monthlyCosts = Math.round(payingUsers * blendedCosts[provider.name]);
        const monthlyProfit = monthlyRevenue - monthlyCosts;
        const annualProfit = monthlyProfit * 12;
        const margin = monthlyProfit / monthlyRevenue;
        const rowClass = provider.isGroq ? ' class="groq-highlight"' : '';

        scaleBody.innerHTML += `
          <tr${rowClass}>
            <td><strong>${provider.name}</strong></td>
            <td>${formatCurrency(monthlyRevenue)}</td>
            <td>${formatCurrency(monthlyCosts)}</td>
            <td class="positive">${formatCurrency(monthlyProfit)}</td>
            <td class="positive">${formatCurrency(annualProfit)}</td>
            <td class="emphasis">${formatPercent(margin)}</td>
          </tr>
        `;
      });

      // Last updated
      document.getElementById('last-updated').textContent = new Date().toLocaleDateString();

      // Set implementation plan date
      const planDateElement = document.getElementById('plan-last-updated');
      if (planDateElement) {
        planDateElement.textContent = new Date().toLocaleDateString();
      }

      // Set post-processing date
      const postProcessingDateElement = document.getElementById('postprocessing-last-updated');
      if (postProcessingDateElement) {
        postProcessingDateElement.textContent = new Date().toLocaleDateString();
      }
    }

    // Run on load
    init();

    // Event listeners for formatting toggle and model selector
    const includeFormattingCheckbox = document.getElementById('include-formatting');
    const formattingOptions = document.getElementById('formatting-options');
    const formattingModelSelector = document.getElementById('formatting-model');

    includeFormattingCheckbox.addEventListener('change', () => {
      // Show/hide formatting options
      formattingOptions.style.display = includeFormattingCheckbox.checked ? 'block' : 'none';
      // Recalculate
      init();
    });

    formattingModelSelector.addEventListener('change', () => {
      // Recalculate when model changes
      init();
    });

    // Sidebar & Navigation
    const sidebar = document.getElementById('sidebar');
    const toggleBtn = document.getElementById('sidebar-toggle');
    const toggleIcon = document.getElementById('toggle-icon');
    const navItems = document.querySelectorAll('.nav-item');
    const pages = document.querySelectorAll('.page');

    // Toggle sidebar
    toggleBtn.addEventListener('click', () => {
      sidebar.classList.toggle('collapsed');
      // Update arrow direction
      toggleIcon.textContent = sidebar.classList.contains('collapsed') ? '▶' : '◀';
    });

    // Navigate between pages
    navItems.forEach(item => {
      item.addEventListener('click', () => {
        const pageId = item.getAttribute('data-page');

        // Update active nav item
        navItems.forEach(nav => nav.classList.remove('active'));
        item.classList.add('active');

        // Show selected page
        pages.forEach(page => page.classList.remove('active'));
        document.getElementById(`page-${pageId}`).classList.add('active');
      });
    });

    // Keyboard shortcut: Ctrl/Cmd + B to toggle sidebar
    document.addEventListener('keydown', (e) => {
      if ((e.ctrlKey || e.metaKey) && e.key === 'b') {
        e.preventDefault();
        sidebar.classList.toggle('collapsed');
      }
    });
  </script>
</body>
</html>
