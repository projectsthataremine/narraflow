# Phase 1: Standalone Audio Pipeline - SUCCESS ‚úÖ

**Date:** October 4, 2025
**Status:** ‚úÖ COMPLETE - All pipelines working
**Location:** `prototype/phase-1/`

---

## üéâ Audio Pipeline

Successfully implemented and tested a **standalone Node.js audio pipeline** that proves the core technology stack works with both file-based and real-time microphone input.

### 1. File-Based Pipeline ‚úÖ

**Test Script:** `test-audio-pipeline.js`

Validates the core VAD ‚Üí Whisper pipeline using pre-recorded audio files.

#### Test Results
- **File:** `sherpa-onnx-whisper-tiny.en/test_wavs/0.wav`
- **Duration:** 6.63 seconds
- **Sample Rate:** 16kHz (mono)

**VAD Detection:**
```
Speech detected: 0.38s - 6.24s (5.85s duration)
‚úÖ Found 1 speech segment
```

**Whisper Transcription:**
```
Input: Audio segment (5.85s)
Output: "After early nightfall, the yellow lamps would light up here
         and there the squalid quarter of the brothels."
```

**Result:** ‚úÖ Accurate transcription

#### Data Flow
```
Audio File (WAV)
    ‚Üì
Load with sherpa_onnx.readWave()
    ‚Üì
Process with VAD (512-sample windows)
    ‚Üì
Detect speech segments
    ‚Üì
Create Whisper stream for each segment
    ‚Üì
Transcribe to text
    ‚Üì
Output final text
```

---

### 2. Microphone-Based Pipeline ‚úÖ

**Test Script:** `test-mic-capture.js`

Validates the complete real-time pipeline: Microphone ‚Üí VAD ‚Üí Whisper

#### Test Results
- **Input Device:** MacBook Pro Microphone
- **Recording Duration:** 5 seconds (timed)
- **Sample Rate:** 16kHz (mono)

**VAD Detection:**
```
‚úÖ Found 2 speech segments
  - Segment 1: 0.40s
  - Segment 2: 1.92s
```

**Whisper Transcription:**
```
Segment 1: "Nice"
Segment 2: "testing testing testing testing..." (repeated)
```

**Result:** ‚úÖ Real-time capture and transcription working

#### Data Flow
```
Microphone (naudiodon2)
    ‚Üì
CircularBuffer (streaming audio management)
    ‚Üì
Process with VAD (512-sample windows)
    ‚Üì
Detect speech segments
    ‚Üì
Timed stop (5 seconds)
    ‚Üì
Flush VAD, extract all segments
    ‚Üì
Create Whisper stream for each segment
    ‚Üì
Transcribe to text
    ‚Üì
Output final text
```

#### Key Components
- **naudiodon2:** PortAudio bindings for Node.js microphone capture
- **CircularBuffer:** `sherpa_onnx.CircularBuffer()` for managing streaming audio
- **Timed recording:** `setTimeout()` to stop after 5 seconds and process

---

## üì¶ Technology Stack

| Component | Package | Version | Status |
|-----------|---------|---------|--------|
| ONNX Runtime | `sherpa-onnx-node` | 1.12.14 | ‚úÖ Working |
| Microphone Capture | `naudiodon2` | 2.5.0 | ‚úÖ Working |
| VAD Model | `silero_vad.onnx` | - | ‚úÖ Working |
| Whisper Model | `whisper-tiny.en` (int8) | - | ‚úÖ Working |
| Platform | macOS (Apple Silicon) | arm64 | ‚úÖ Working |

---

## üîë Key Code Patterns

### 1. Initialize VAD
```javascript
const vad = new sherpa_onnx.Vad(vadConfig, bufferSizeInSeconds);
```

### 2. Process Audio with VAD (File-based)
```javascript
for (let i = 0; i < wave.samples.length; i += windowSize) {
  const end = Math.min(i + windowSize, wave.samples.length);
  vad.acceptWaveform(wave.samples.subarray(start, end));
}
vad.flush();
```

### 3. Process Audio with VAD (Microphone)
```javascript
const buffer = new sherpa_onnx.CircularBuffer(bufferSizeInSeconds * sampleRate);

ai.on('data', data => {
  const samples = new Float32Array(data.buffer);
  buffer.push(samples);

  while (buffer.size() > windowSize) {
    const chunk = buffer.get(buffer.head(), windowSize);
    buffer.pop(windowSize);
    vad.acceptWaveform(chunk);
  }
});
```

### 4. Set Up Microphone
```javascript
const ai = new portAudio.AudioIO({
  inOptions: {
    channelCount: 1,
    closeOnError: true,
    deviceId: -1, // default device
    sampleFormat: portAudio.SampleFormatFloat32,
    sampleRate: 16000,
  }
});

ai.start();
```

### 5. Extract Speech Segments
```javascript
vad.flush();

while (!vad.isEmpty()) {
  const segment = vad.front();
  vad.pop();
  speechSegments.push(segment);
}
```

### 6. Transcribe with Whisper
```javascript
const recognizer = new sherpa_onnx.OfflineRecognizer(recognizerConfig);
const stream = recognizer.createStream();

stream.acceptWaveform({
  sampleRate: wave.sampleRate,
  samples: segment.samples,
});

recognizer.decode(stream);
const result = recognizer.getResult(stream);
console.log(result.text); // Transcribed text
```

---

## ‚öôÔ∏è Configuration

### VAD Config
```javascript
{
  sileroVad: {
    model: 'models/silero_vad.onnx',
    threshold: 0.5,
    minSilenceDuration: 0.5,
    minSpeechDuration: 0.25,
    maxSpeechDuration: 5,
    windowSize: 512,
  },
  sampleRate: 16000,
  numThreads: 1,
}
```

### Whisper Config
```javascript
{
  modelConfig: {
    whisper: {
      encoder: 'tiny.en-encoder.int8.onnx',
      decoder: 'tiny.en-decoder.int8.onnx',
    },
    tokens: 'tiny.en-tokens.txt',
    numThreads: 2,
    provider: 'cpu',
  },
  featConfig: {
    sampleRate: 16000,
    featureDim: 80,
  },
}
```

---

## üöÄ Running the Tests

### Prerequisites
```bash
cd prototype/phase-1
npm install  # Installs sherpa-onnx-node and naudiodon2
```

### Execution

#### File-Based Test
```bash
# Option 1: Using helper script (auto-detects Intel/ARM)
./run-test.sh

# Option 2: Manual (Apple Silicon)
export DYLD_LIBRARY_PATH=$PWD/node_modules/sherpa-onnx-darwin-arm64:$DYLD_LIBRARY_PATH
node test-audio-pipeline.js

# Option 3: Manual (Intel)
export DYLD_LIBRARY_PATH=$PWD/node_modules/sherpa-onnx-darwin-x64:$DYLD_LIBRARY_PATH
node test-audio-pipeline.js
```

#### Microphone Test
```bash
# Using helper script (auto-detects Intel/ARM)
./run-test.sh mic

# Manual (Apple Silicon)
export DYLD_LIBRARY_PATH=$PWD/node_modules/sherpa-onnx-darwin-arm64:$DYLD_LIBRARY_PATH
node test-mic-capture.js

# Manual (Intel)
export DYLD_LIBRARY_PATH=$PWD/node_modules/sherpa-onnx-darwin-x64:$DYLD_LIBRARY_PATH
node test-mic-capture.js
```

---

## üìÅ Files Created

```
prototype/phase-1/
‚îú‚îÄ‚îÄ test-audio-pipeline.js    # File-based test (working!)
‚îú‚îÄ‚îÄ test-mic-capture.js        # Microphone test (working!)
‚îú‚îÄ‚îÄ run-test.sh                # Helper script for cross-platform execution
‚îú‚îÄ‚îÄ package.json               # Dependencies
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ silero_vad.onnx       # VAD model
‚îÇ   ‚îî‚îÄ‚îÄ sherpa-onnx-whisper-tiny.en/
‚îÇ       ‚îú‚îÄ‚îÄ tiny.en-encoder.int8.onnx
‚îÇ       ‚îú‚îÄ‚îÄ tiny.en-decoder.int8.onnx
‚îÇ       ‚îú‚îÄ‚îÄ tiny.en-tokens.txt
‚îÇ       ‚îî‚îÄ‚îÄ test_wavs/         # Sample audio files
‚îî‚îÄ‚îÄ README.md                  # Setup instructions
```

---

## üéØ Lessons Learned

### 1. API Differences from Examples
- ‚úÖ Use `new sherpa_onnx.Vad()` not `createVad()`
- ‚úÖ Use `new sherpa_onnx.OfflineRecognizer()` not `createOfflineRecognizer()`
- ‚úÖ Use `vad.isEmpty()` not `vad.empty()`
- ‚úÖ Use CommonJS (`require`) not ES modules (`import`)

### 2. Library Path Critical
- Must set `DYLD_LIBRARY_PATH` on macOS
- Different paths for Intel vs Apple Silicon
- Helper script (`run-test.sh`) auto-detects architecture

### 3. Model Formats
- Used **int8 quantized** Whisper models for speed
- Works perfectly on CPU (no GPU needed)
- 16kHz sample rate required

### 4. Microphone Integration
- **naudiodon2** provides PortAudio bindings for Node.js
- **CircularBuffer** essential for streaming audio management
- Process audio in windows (512 samples) for real-time VAD
- Timed recording with `setTimeout()` for controlled tests

---

## üîú Next Steps

### ‚úÖ Phase 1 Complete
- [x] File-based VAD ‚Üí Whisper pipeline
- [x] Microphone capture for real-time audio
- [x] Test with live speech input
- [x] Validate complete pipeline

### Phase 2 (Electron Integration)
- [ ] Move working pipeline into Electron worker thread
- [ ] Implement IPC communication (main ‚Üî worker)
- [ ] Connect to existing UI (pill overlay)
- [ ] Add clipboard/paste functionality

### Phase 3 (Complete MVP)
- [ ] Global hotkey (Command+B)
- [ ] Full end-to-end: Mic ‚Üí VAD ‚Üí Whisper ‚Üí Clipboard ‚Üí Paste
- [ ] Performance optimization
- [ ] Error handling and edge cases

---

## üìä Performance Metrics

| Metric | File-Based | Microphone |
|--------|-----------|------------|
| VAD Detection Time | Instant | Real-time |
| Whisper Transcription (5-6s audio) | ~2-3 seconds | ~2-3 seconds |
| Accuracy | ‚úÖ Excellent | ‚úÖ Excellent |
| Memory Usage | Acceptable (int8 models) | Acceptable |
| Cross-Platform | ‚úÖ Works on Apple Silicon | ‚úÖ Works on Apple Silicon |

---

## üéì Confidence Level: 100%

We now have **complete confidence** that:
- ‚úÖ The technology stack is viable
- ‚úÖ VAD works for speech detection (file & real-time)
- ‚úÖ Whisper works for transcription
- ‚úÖ Microphone capture works with naudiodon2
- ‚úÖ CircularBuffer handles streaming audio correctly
- ‚úÖ Models run efficiently on macOS
- ‚úÖ No "black boxes" - we understand the entire pipeline

**Phase 1 is COMPLETE. Ready for Phase 2.**

---

## üß™ LLM Text Cleanup Discovery

**Date:** October 5, 2025
**Status:** Research complete, implementation pending

### Overview

After Whisper transcription, we need to clean up the raw text output. Whisper already does a decent job with basic grammar and punctuation, but needs help with:
- Formatting lists (adding colons, bullets, or line breaks)
- Fixing awkward repetitions ("so so", "gonna gonna", "if if")
- Cleaning up restarts and false starts ("I want to I need to" ‚Üí "I need to")
- Removing excessive filler words ("you know", "like", "um")
- Fixing obvious typos/transcription errors

### Model Selection: Llama 3.1 8B

**Testing Results:**
- ‚ùå **Llama 3.2 3B** - Too small, unreliable
  - Hallucinated content ("This is already clean" ‚Üí paragraph about favorite books)
  - Removed important context ("I have a to-do list" ‚Üí deleted entirely)
  - Changed numbers and facts
- ‚úÖ **Llama 3.1 8B** - Reliable and consistent
  - Preserves context and meaning
  - Fixes repetitions and filler words appropriately
  - Handles natural speech patterns well
  - Model size: ~4.7GB

**Test Script:** `prototype/phase-1/test-cleanup.js`

### System Prompt Design

Final prompt that works well:

```
You are a text formatting tool in an audio transcription pipeline.
The text you receive is already transcribed speech from Whisper.
Your ONLY job is to output a cleaned version.

WHAT TO FIX:
- Fix awkward repetitions (e.g., "so so" ‚Üí "so", "gonna gonna" ‚Üí "gonna")
- Clean up restarts and false starts (e.g., "I want to I need to" ‚Üí "I need to")
- Remove excessive filler words when they disrupt flow
- Format lists properly (add colons, bullets, or line breaks)
- Fix obvious typos/transcription errors

WHAT TO PRESERVE:
- ALL introductory phrases and context
- The complete sentence structure and meaning
- Natural conversational flow

CRITICAL: Output ONLY the cleaned text. NO explanations or commentary.
```

### Deployment Strategy: CoreML vs llama.cpp

**Decision: Use CoreML for macOS Apple Silicon**

#### Platform Support Analysis

**Intel Mac Timeline:**
- Last Intel Mac sold: Mac Pro (June 2023)
- Last consumer Intel Mac: MacBook Pro 13" (June 2022)
- Transition started: November 2020 (M1 launch)
- **Result:** 95%+ of active Mac users are on Apple Silicon

**Primary: CoreML (Apple Silicon M1/M2/M3)**
- ‚úÖ Much faster inference (uses Neural Engine)
- ‚úÖ Lower power consumption
- ‚úÖ Native macOS integration
- ‚úÖ ~4.5GB model file (CoreML format)
- ‚ùå macOS only (not cross-platform)

**Fallback: llama.cpp (Intel Macs)**
- ‚úÖ Works on older Intel Macs
- ‚úÖ Cross-platform (if we add Windows/Linux later)
- ‚ùå Slower than CoreML on Apple Silicon
- ‚ùå Higher power consumption

**Or skip Intel support entirely** - "Requires Apple Silicon Mac"

### Cost Analysis: Why Local Inference is Required

**Cloud API Economics Don't Work:**

If using Claude/GPT APIs at $15/month subscription:
- Cleanup: ~300-400 tokens per use
- Power user: ~100 cleanups/day
- Monthly usage: ~1.2M tokens
- **API cost: $3.60-$36/month per user** (depending on model)

A power user costs $20-30/month in API fees alone - **not sustainable at $15/month pricing**.

**Conclusion:** Must run LLM locally like Whispr Flow likely does.

### Product Strategy: Optional Download

**Two-Tier Approach:**

1. **Basic Mode (Default)**
   - No LLM download required
   - Simple rule-based cleanup (capitalization, basic punctuation)
   - Tiny footprint - just Whisper model (~74MB)
   - Fast, works offline immediately

2. **Enhanced Cleanup (Optional)**
   - Checkbox: "Enable AI text cleanup (requires 4.7GB download)"
   - Downloads Llama 3.1 8B CoreML model on first use
   - Progress bar for download
   - Fixes repetitions, filler words, formats lists nicely

**Total Size:**
- Whisper tiny.en: ~74MB (required)
- Llama 3.1 8B: ~4.7GB (optional)

**Self-Contained App:**
- No external dependencies (no Ollama installation)
- Use CoreML (macOS native) or bundle llama.cpp
- Download models within the app
- Everything runs locally

### Implementation Plan

**Current Prototype:**
- Uses Ollama for testing (`llama3.1:8b`)
- Test script: `prototype/phase-1/test-cleanup.js`
- Validates prompt quality and model behavior

**Production App:**
1. Convert Llama 3.1 8B to CoreML format (~4.5GB .mlpackage file)
2. Add settings toggle: "Enable Enhanced Cleanup"
3. Download CoreML model to app data directory on first enable
4. Use CoreML APIs (or Node.js bridge) to run inference
5. Fall back to basic cleanup if:
   - User disabled enhanced mode
   - Model not downloaded yet
   - Running on Intel Mac (optional)

### Technical Integration Options

**Option 1: CoreML Native (Recommended)**
- Use Swift/Objective-C to interface with CoreML
- Create Node.js native module bridge
- Best performance on Apple Silicon

**Option 2: Existing Libraries**
- Research `node-coreml` or similar bindings
- Use what community has built (LM Studio, etc.)

**Option 3: Child Process**
- Write Swift CLI tool for CoreML inference
- Call from Node.js via child process
- Simple but less efficient

### What Whispr Flow Likely Uses

**Analysis:**
- Network traffic observed during use
- But economics suggest local inference
- Most likely: CoreML + network for auth/telemetry

**Their likely tech stack:**
- CoreML for LLM inference (local)
- Network calls for:
  - License/auth verification
  - Usage telemetry
  - Model updates/downloads
  - Sync settings

### Test Results Summary

**11 test cases covering:**
- Simple lists with context
- Natural speech with filler words
- Stream of consciousness with restarts
- Mid-sentence corrections
- Repeated words and phrases
- Already clean text (should be unchanged)

**Llama 3.1 8B performance:** ‚úÖ Excellent
- Preserves context ("I have a to-do list")
- Fixes repetitions naturally
- Handles filler words appropriately
- Doesn't hallucinate or add content
- Outputs clean text without commentary

---

---

## üöÄ Next Steps: Phase 2 Electron Integration

**Status:** Ready to start (Phase 1 complete)
**Last updated:** 2025-10-09

### Phase 2A: Core Audio Pipeline Integration

#### 1. Install Dependencies in Main Project
- [ ] Add `sherpa-onnx-node` to root `package.json`
- [ ] Add `naudiodon2` to root `package.json`
- [ ] Copy ONNX models from `prototype/phase-1/models/` to `resources/models/`
- [ ] Set up library paths for macOS (handle both arm64 and x64)
- [ ] Update Electron builder config to bundle native dependencies

#### 2. Integrate Audio Pipeline into Worker Thread
- [ ] Port VAD initialization from `prototype/phase-1/test-mic-capture.js`
- [ ] Port Whisper initialization from prototype
- [ ] Replace stub transcription in `src/worker/whisper.ts` with real sherpa-onnx
- [ ] Update `src/worker/pipeline.ts` to use real VAD ‚Üí Whisper flow
- [ ] Test worker thread in isolation (send mock audio, verify transcription)

#### 3. Connect Microphone Capture
- [ ] Decide: Run `naudiodon2` in worker thread OR main process
  - **Option A:** Worker thread (simpler IPC, but native audio in worker)
  - **Option B:** Main process (capture audio ‚Üí send chunks via IPC to worker)
- [ ] Implement CircularBuffer pattern from prototype
- [ ] Stream audio chunks through VAD in 512-sample windows
- [ ] Send real-time VAD probabilities back to UI via IPC

#### 4. Wire Up Real-Time UI Updates
- [ ] Update IPC to send VAD probabilities during recording
- [ ] Update pill UI to reflect real VAD state (not mocked)
- [ ] Test: Press Cmd+B ‚Üí pill shows "loading" ‚Üí "silent" ‚Üí "talking" (based on real audio)

#### 5. Add Clipboard + Paste Functionality
- [ ] Implement `src/paste/clipboard.ts` - copy text to clipboard
- [ ] Implement `src/paste/paste.ts` - simulate Cmd+V using `@nut-tree/nut-js` or `robotjs`
- [ ] Wire up to IPC handlers in `src/main/ipc-handlers.ts`
- [ ] Test: Transcription ‚Üí clipboard ‚Üí paste into focused field

#### 6. End-to-End Testing
- [ ] Test full flow: Cmd+B ‚Üí record ‚Üí speak ‚Üí release ‚Üí transcribe ‚Üí paste
- [ ] Verify pill states animate correctly
- [ ] Verify text appears in focused field
- [ ] Test edge cases: no focused field (clipboard only), rapid key presses, etc.

### Phase 2B: LLM Text Cleanup (Optional)

- [ ] Research CoreML integration with Node.js/Electron
- [ ] Convert Llama 3.1 8B to CoreML format
- [ ] Add settings UI for "Enable Enhanced Cleanup" toggle
- [ ] Implement model download within app
- [ ] Add LLM cleanup step in worker pipeline
- [ ] Fall back to basic cleanup if disabled/unavailable

### Phase 3: Polish & Optimization

- [ ] Performance optimization (reduce latency)
- [ ] Comprehensive error handling
- [ ] Settings window (hotkey customization, model preferences)
- [ ] App packaging and distribution
- [ ] Documentation and user guide

---

## üìù References

- [sherpa-onnx GitHub](https://github.com/k2-fsa/sherpa-onnx)
- [sherpa-onnx Node.js Examples](https://github.com/k2-fsa/sherpa-onnx/tree/master/nodejs-addon-examples)
- [Silero VAD](https://github.com/snakers4/silero-vad)
- [naudiodon2 (PortAudio for Node.js)](https://www.npmjs.com/package/naudiodon2)
- Working file test: `prototype/phase-1/test-audio-pipeline.js`
- Working mic test: `prototype/phase-1/test-mic-capture.js`
- LLM cleanup test: `prototype/phase-1/test-cleanup.js`
